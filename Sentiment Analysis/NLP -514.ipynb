{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b27be99",
   "metadata": {},
   "source": [
    "# 514-SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2272f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbc3717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5946f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\91917\\Downloads\\Restaurant_Reviews.tsv\"\n",
    "data = pd.read_csv(file_path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e8d9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7e1cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords and lemmatize each token\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(lemmatized_tokens), lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973cd04",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f02d0786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\91917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')  # Ensures WordNet itself is also available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d1e2df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure the required corpora are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = text.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in stop_words]\n",
    "    return ' '.join(lemmatized_tokens), lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc563cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cleaned_Review'], data['Lemmatized_Tokens'] = zip(*data['Review'].apply(preprocess_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7b64259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0                           Wow... Loved this place.   \n",
      "1                                 Crust is not good.   \n",
      "2          Not tasty and the texture was just nasty.   \n",
      "3  Stopped by during the late May bank holiday of...   \n",
      "4  The selection on the menu was great and so wer...   \n",
      "\n",
      "                                      Cleaned_Review  \\\n",
      "0                                Wow... Loved place.   \n",
      "1                                        Crust good.   \n",
      "2                               tasty texture nasty.   \n",
      "3  Stopped late May bank holiday Rick Steve recom...   \n",
      "4                       selection menu great prices.   \n",
      "\n",
      "                                   Lemmatized_Tokens  \n",
      "0                            [Wow..., Loved, place.]  \n",
      "1                                     [Crust, good.]  \n",
      "2                           [tasty, texture, nasty.]  \n",
      "3  [Stopped, late, May, bank, holiday, Rick, Stev...  \n",
      "4                  [selection, menu, great, prices.]  \n"
     ]
    }
   ],
   "source": [
    "print(data[['Review', 'Cleaned_Review', 'Lemmatized_Tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bb768b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  \\\n",
      "995  I think food should have flavor and texture an...   \n",
      "996                           Appetite instantly gone.   \n",
      "997  Overall I was not impressed and would not go b...   \n",
      "998  The whole experience was underwhelming, and I ...   \n",
      "999  Then, as if I hadn't wasted enough of my life ...   \n",
      "\n",
      "                                        Cleaned_Review  \\\n",
      "995                 think food flavor texture lacking.   \n",
      "996                           Appetite instantly gone.   \n",
      "997                   Overall impressed would go back.   \n",
      "998  whole experience underwhelming, think we'll go...   \n",
      "999  Then, wasted enough life there, poured salt wo...   \n",
      "\n",
      "                                     Lemmatized_Tokens  \n",
      "995           [think, food, flavor, texture, lacking.]  \n",
      "996                       [Appetite, instantly, gone.]  \n",
      "997             [Overall, impressed, would, go, back.]  \n",
      "998  [whole, experience, underwhelming,, think, we'...  \n",
      "999  [Then,, wasted, enough, life, there,, poured, ...  \n"
     ]
    }
   ],
   "source": [
    "print(data[['Review', 'Cleaned_Review', 'Lemmatized_Tokens']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c7f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
